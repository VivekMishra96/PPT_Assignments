{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0789de3",
   "metadata": {},
   "source": [
    "Q-4. Take any text file and now your task is to Text Summarization without using hugging transformer library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45c29235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-9.Usingwisheryouneedtranscribeanyaudiofileandthenyouneedtoconvertthataudiofileintotextfileandnowconvertthattextfileintoaudiofileofdifferentlanguag e.\n",
      "Q-10.BuildawholeEnd-EndapianddeployitonHeroku/railwayssothetaskisthatyouneedbuildaAuto-CorrectionoftextusingNLPNote:onlyJupyternotebookisnotallowedfrom5thquestion\"\n",
      "\"←\n",
      "S u b m i s s i o n\n",
      "P r o c e s s\n",
      "→\n",
      "There\n",
      "are\n",
      "Two\n",
      "Types\n",
      "of\n",
      "Questions\n",
      "Theory\n",
      "based\n",
      "Question\n",
      "and\n",
      "Project-based\n",
      "(where\n",
      "you\n",
      "actually\n",
      "have\n",
      "to\n",
      "code)\n",
      "First\n",
      "of\n",
      "all,\n",
      "You\n",
      "have\n",
      "to\n",
      "create\n",
      "an\n",
      "Google\n",
      "doc,\n",
      "where\n",
      "you\n",
      "will\n",
      "add\n",
      "answers\n",
      "of\n",
      "all\n",
      "the\n",
      "questions\n",
      "If\n",
      "you\n",
      "are\n",
      "attempting\n",
      "a\n",
      "question\n",
      "in\n",
      "which\n",
      "you\n",
      "have\n",
      "to\n",
      "write\n",
      "code,\n",
      "so\n",
      "create\n",
      "a\n",
      "repo\n",
      "push\n",
      "your\n",
      "code\n",
      "to\n",
      "repo\n",
      "and\n",
      "copy\n",
      "the\n",
      "link\n",
      "of\n",
      "repo\n",
      "and\n",
      "add\n",
      "it\n",
      "into\n",
      "docs\n",
      "as\n",
      "shown\n",
      "below\n",
      "E g .\n",
      "A n s w e r .\n",
      "6\n",
      "P y t h o n\n",
      "-\n",
      ">\n",
      "G i t H u b\n",
      "r e p o\n",
      "l i n k\n",
      "N o t e :\n",
      "●\n",
      "I f\n",
      "y o u\n",
      "a r e\n",
      "b u i l d i n g\n",
      "a n y\n",
      "E n d\n",
      "t o\n",
      "e n d\n",
      "p r o j e c t\n",
      "t r y\n",
      "t o\n",
      "w r i t e\n",
      "c o d e\n",
      "i n\n",
      ". p y\n",
      "f i l e\n",
      "●\n",
      "I f\n",
      "y o u\n",
      "a r e\n",
      "o n l y\n",
      "a n a l y z i n g\n",
      "o r\n",
      "d o i n g\n",
      "E D A\n",
      "u s e\n",
      ". i p y n b\n",
      "f i l e\n",
      "If\n",
      "you\n",
      "are\n",
      "attempting\n",
      "a\n",
      "theory-based\n",
      "question\n",
      "then\n",
      "you\n",
      "have\n",
      "to\n",
      "add\n",
      "the\n",
      "answer\n",
      "in\n",
      "the\n",
      "same\n",
      "google\n",
      "docs\n",
      "as\n",
      "it's\n",
      "Then\n",
      "submit\n",
      "that\n",
      "final\n",
      "link\n",
      "(google\n",
      "doc\n",
      "link\n",
      "which\n",
      "has\n",
      "all\n",
      "the\n",
      "answers)\" \"Sincethisisadeeplearningproject,theuseofGPUs,andhoweffectivelyareyouusingthemtooptimizeforcostandtrainingtimeshouldalsobetakenintoconsideration.6.Inthedeploymentpipeline,youwouldbedesigninghoweffectivelyandefficientlyyouaredeployingthemodelinthecloud,7.Intheinferencepipeline,considerthecostofinferenceanditsoptimization\n",
      "relatedtocomputingresourcesandhandlingexternaltraffic8.Youcanuseanytooltodesignthearchitecture9.Domentiontheprosandconsofyourarchitectureandhowmuchfurtheritcanbeoptimizedanditstradeoffs.10.Doincludearetrainingapproachaswell.11.TrytoincludemanagedAWSresourcesfordeeplearninglikeAWSTextract,AWSSagemaker,etc.,andnotjustgeneral-purposecomputeresourceslikeS3,EC2,etc.Trytomixthebestofbothservices\n",
      "Question5-\n",
      "InQuestion4,youhavedesignedthearchitectureforanobjectdetectionusecaseleveragingAWSCloud,similarly ,hereyouwillbedesigningforDocumen tClassific ationusecaseleveragingAzureCloudservices.Note-1.MostofthepointsarethesameasinQuestion4,justcloudserviceswillchange\"\n",
      "\"Comput erVision\n",
      "T o t a l\n",
      "M a r k s :\n",
      "2 0 0\n",
      "E a c h\n",
      "q u e s t i o n\n",
      "2 0\n",
      "m a r k s\n",
      "Question1-Trainadeeplearningmodelwhichwouldclassifythevegetablesbasedontheimagesprovided.Thedatasetcanbeaccessedfromthegivenlink. Q-3.Acompanywantstopredictthesalesofitsproductbasedonthemoneyspentondifferentplatformsformarketing.Theywantyoutofigureouthowtheycanspendmoneyonmarketinginthefutureinsuchawaythattheycanincreasetheirprofitasmuchaspossiblebuilt-indockerandusesomelibrarytodisplaythatinfrontendDatasetThisistheDatasetYoucanusethisdatasetforthisquestion.Note:UseonlyDask\n",
      "Q-4.Takeany3questionsanddeploythemtoAWSusingGitHubActionsandshowademolink\n",
      "Q-5.Takeany3questionsanddeploythemtoAWSusingCircle-CIandshowademolink\"\n",
      "\"DeepLearning\n",
      "T o t a l\n",
      "M a r k s :\n",
      "1 0 0\n",
      "E a c h\n",
      "q u e s t i o n\n",
      "2 0\n",
      "m a r k s\n",
      "Question1-Implemen t3differentCNNarchitectureswithacomparisontablefortheMNSITdatasetusingtheTensorflo wlibrary.Note-1.Themodelparametersforeacharchitectureshouldnotbemorethan8000parameters2.Codecommentsshouldbegivenforpropercodeunderstanding.3.Theminimumaccuracyforeachaccuracyshouldbeatleast96%\n",
      "Question2-Implemen t5differentCNNarchitectureswithacomparisontableforCIFAR10datasetusingthePyTorchlibraryNote-1.Themodelparametersforeacharchitectureshouldnotbemorethan10000parameters\n",
      "2Codecommentsshouldbegivenforpropercodeunderstanding\n",
      "Question3-TrainaPureCNNwithlessthan10000trainableparametersusingtheMNISTDatasethavingminimumvalidationaccuracyof99.40%Note-1.Codecommentsshouldbegivenforpropercodeunderstanding.2.Implemen tinbothPyTorchandTensorflo wrespectiv ely\n",
      "Question4-Designanend-to-endsolutionwithdiagramsforobjectdetectionusecasesleveragingAWScloudservicesandopen-sour cetechNote-1.YouneedtousebothAWScloudservicesandopen-sour cetechtodesigntheentiresolution2.Thepipelineshouldconsistofadatapipeline,mlpipeline,deploymentpipeline,andinferencepipeline.3.Inthedatapipeline,youwouldbedesigninghowtogetthedatafromexternalorexistingsourcesandtechusedforthesame4.Inthemlpipeline,youwouldbedesigninghowtotrainthemodel,andwhatallalgorithms,techniques,etc.wouldyoubeusing.Again,techusedforthesame5.\"\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from collections import defaultdict\n",
    "\n",
    "# Read the text file into a Python variable.\n",
    "text = open(\"data.csv\", \"r\").read()\n",
    "\n",
    "# Tokenize the text into sentences.\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Tokenize the sentences into words.\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Remove stop words.\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "filtered_words = [word for word in words if word.casefold() not in stop_words]\n",
    "\n",
    "# Calculate word frequencies.\n",
    "word_frequencies = defaultdict(int)\n",
    "for word in filtered_words:\n",
    "    word_frequencies[word] += 1\n",
    "\n",
    "# Calculate sentence scores based on word frequencies.\n",
    "sentence_scores = defaultdict(int)\n",
    "for sentence in sentences:\n",
    "    for word in word_tokenize(sentence):\n",
    "        if word.casefold() in word_frequencies:\n",
    "            sentence_scores[sentence] += word_frequencies[word.casefold()]\n",
    "\n",
    "# Sort sentences based on their scores in descending order.\n",
    "sorted_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)\n",
    "\n",
    "# Extract the top sentences to form a summary.\n",
    "num_sentences = min(3, len(sorted_sentences))  # You can adjust the number of sentences in the summary.\n",
    "top_sentences = sorted_sentences[:num_sentences]\n",
    "\n",
    "# Join the extracted sentences together to form the summary.\n",
    "summary = \" \".join(top_sentences)\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b53a98b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
